{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np;\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as album\n",
    "import math\n",
    "\n",
    "from sklearn import model_selection\n",
    "from scipy import ndimage\n",
    "from typing import Tuple, List\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import joblib\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "class CaImagesDataset(Dataset):\n",
    "    \"\"\"CA Images dataset.\"\"\"\n",
    "    # load the dataset\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "\n",
    "    # number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    # get a sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x[idx], self.y[idx]]\n",
    "    \n",
    "class NeuralNetworkClassifier(nn.Module):\n",
    "    \"\"\"Neural network with LSTM layer and fully connected layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkClassifier,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, \n",
    "                            hidden_size=2,\n",
    "                            bidirectional=False,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True\n",
    "                            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,_status = self.lstm(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"/Users/huayinluo/Desktop/code/catracking-1/data\"\n",
    "video_dir = os.path.join(data_dir, \"imgs\")\n",
    "position_dir = os.path.join(data_dir, \"positions\")\n",
    "model_dir = \"/Users/huayinluo/Desktop/code/catracking-1/models/lstm\"\n",
    "img_dir = \"/Users/huayinluo/Desktop/code/catracking-1/images\"\n",
    "results_dir = \"/Users/huayinluo/Desktop/code/catracking-1/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2571, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = \"11408\"\n",
    "AVA_positions = np.load(os.path.join(position_dir, f\"AVA_{video}.mat.npy\"))\n",
    "AVB_positions = np.load(os.path.join(position_dir, f\"AVB_{video}.mat.npy\"))\n",
    "AVA_positions.shape, AVB_positions.shape\n",
    "all_positions = np.stack((AVA_positions, AVB_positions))\n",
    "all_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117.        ,  92.5       ],\n",
       "       [118.08333333,  87.66666667],\n",
       "       [117.5       ,  82.5       ],\n",
       "       ...,\n",
       "       [132.05882353, 302.41176471],\n",
       "       [130.23076923, 302.84615385],\n",
       "       [130.29166667, 302.70833333]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109.5       , 108.        ],\n",
       "       [111.        , 105.5       ],\n",
       "       [110.5       , 100.875     ],\n",
       "       ...,\n",
       "       [135.5       , 314.5       ],\n",
       "       [135.5       , 315.5       ],\n",
       "       [136.66666667, 316.33333333]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 11408...\n",
      "Loading 11409...\n",
      "Loading 11410...\n",
      "Loading 11411...\n",
      "Finished loading images and positions: 7 videos, 4 positions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save all video arrays and positions in dictionary\n",
    "videos = ['11408', '11409', \"11410\", '11411']\n",
    "positions_dct={}\n",
    "for video in videos:\n",
    "  AVA_positions = np.load(os.path.join(position_dir, f\"AVA_{video}.mat.npy\"))\n",
    "  AVB_positions = np.load(os.path.join(position_dir, f\"AVB_{video}.mat.npy\"))\n",
    "  all_neurons_positions = np.stack((AVA_positions, AVB_positions))\n",
    "  positions_dct[video] = all_neurons_positions\n",
    "  print(f\"Loading {video}...\")\n",
    "print(f\"Finished loading images and positions: {len(imgs_dct)} videos, {len(positions_dct)} positions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test/Train split complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test/train split (# Add 80% of each video to training set, 20% to testing set)\n",
    "df_train_lst = []\n",
    "df_test_lst = []\n",
    "for video in videos:\n",
    "  all_positions = positions_dct[video]\n",
    "  height, width = cv2.imread(f\"/Users/huayinluo/Desktop/code/catracking-1/images/original/{video}/0.png\").shape[:2]\n",
    "  norm_positions = np.multiply(all_positions, [1/width, 1/height])\n",
    "  split = math.floor(norm_positions.shape[1]*0.8)\n",
    "  df_train_lst.append(norm_positions[:, :split, :])\n",
    "  df_test_lst.append(norm_positions[:, split:, :])\n",
    "print(\"Test/Train split complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2056"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_lst[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava, avb = df_train_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2056, 2), (2056, 2))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ava.shape, avb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.31034483, 0.1911157 ],\n",
       "        [0.31321839, 0.18112948]]),\n",
       " array([[0.29045093, 0.2231405 ],\n",
       "        [0.29442971, 0.21797521]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ava[:2], avb[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = np.zeros(10)\n",
    "lst[:2] = np.ones(2)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1.]\n",
      "[1. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "lst = np.concatenate((np.zeros(2), np.ones(n-2)))\n",
    "print(lst)\n",
    "random.shuffle(lst)\n",
    "\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2846566804.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[100], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    for j in range\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "all_lst = []\n",
    "for i in range(2):\n",
    "    lst = np.zeros((2, 2))\n",
    "    for j in range\n",
    "    lst[i] = ava[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31034483, 0.1911157 ],\n",
       "       [0.31321839, 0.18112948]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
